{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"H_5zgCg4FZLS"},"outputs":[],"source":["#TruongNguyen for De novo Promoters\n","\n","import pandas as pd\n","import numpy as np\n","import csv, os\n","from numpy import savetxt\n","import random\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","\n","# Load train dataset\n","df = pd.read_csv('')\n","\n","# Assuming the last column is the label/dependent output\n","X_train = TSS.iloc[:, :-1]  # Exclude the last column\n","y_train = TSS.iloc[:, -1]   # Use the last column as the label\n","y_train = y_train.copy()\n","X = X_train.dropna(axis='columns')\n","\n","# Defining various steps required for the genetic algorithm\n","def initilization_of_population(size, n_feat):\n","    population = []\n","    for i in range(size):\n","        chromosome = np.ones(n_feat, dtype=np.bool)\n","        chromosome[:int(0.3 * n_feat)] = False\n","        np.random.shuffle(chromosome)\n","        population.append(chromosome)\n","    return population\n","\n","def fitness_score(population):\n","    scores = []\n","    newtp = []\n","    newfp = []\n","    newtn = []\n","    newfn = []\n","    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","    for chromosome in population:\n","        tp = []\n","        fp = []\n","        tn = []\n","        fn = []\n","        acc = []\n","        for train, test in kfold.split(X, y_train):\n","            model = RandomForestClassifier()\n","            model.fit(X.iloc[train, chromosome], y_train[train])\n","            true_labels = np.asarray(y_train[test])\n","            predictions = model.predict(X.iloc[test, chromosome])\n","\n","            ntp, nfn, ntn, nfp = confusion_matrix(true_labels, predictions).ravel()\n","            tp.append(ntp)\n","            fp.append(nfp)\n","            tn.append(ntn)\n","            fn.append(nfn)\n","            acc.append(accuracy_score(true_labels, predictions))\n","\n","        scores.append(np.mean(acc))\n","        newtp.append(np.sum(tp))\n","        newfp.append(np.sum(fp))\n","        newtn.append(np.sum(tn))\n","        newfn.append(np.sum(fn))\n","\n","    scores, population = np.array(scores), np.array(population)\n","\n","    weights = scores / np.sum(scores)\n","    newtp, newfp, newtn, newfn = np.array(newtp), np.array(newfp), np.array(newtn), np.array(newfn)\n","    inds = np.argsort(scores)\n","\n","    return (\n","        list(scores[inds][::-1]),\n","        list(population[inds, :][::-1]),\n","        list(weights[inds][::-1]),\n","        list(newtp[inds][::-1]),\n","        list(newfp[inds][::-1]),\n","        list(newtn[inds][::-1]),\n","        list(newfn[inds][::-1]),\n","    )\n","\n","def selection(pop_after_fit, weights, k):\n","    pop_after_sel = []\n","    selected_pop = random.choices(pop_after_fit, weights=weights, k=k)\n","    for t in selected_pop:\n","        pop_after_sel.append(t)\n","    return pop_after_sel\n","\n","def crossover(p1, p2, crossover_rate):\n","    # Children are copies of parents by default\n","    c1, c2 = p1.copy(), p2.copy()\n","    # Check for recombination\n","    if random.random() < crossover_rate:\n","        # Select crossover point that is not on the end of the string\n","        pt = random.randint(1, len(p1) - 2)\n","        # Perform crossover\n","        c1 = np.concatenate((p1[:pt], p2[pt:]))\n","        c2 = np.concatenate((p2[:pt], p1[pt:]))\n","    return [c1, c2]\n","\n","def mutation(chromosome, mutation_rate):\n","    for i in range(len(chromosome)):\n","        # Check for a mutation\n","        if random.random() < mutation_rate:\n","            # Flip the bit\n","            chromosome[i] = not chromosome[i]\n","\n","def generations(size, n_feat, crossover_rate, mutation_rate, n_gen):\n","    best_chromo = []\n","    best_score = []\n","    population_nextgen = initilization_of_population(size, n_feat)\n","\n","    for i in range(n_gen):\n","        scores, pop_after_fit, weights, tp, fp, tn, fn = fitness_score(population_nextgen)\n","        score = scores[0]\n","        print('gen', i, score)\n","\n","        k = size - 2\n","        pop_after_sel = selection(pop_after_fit, weights, k)\n","\n","        # Create the next generation\n","        children = []\n","        for i in range(0, len(pop_after_sel), 2):\n","            # Get selected parents in pairs\n","            p1, p2 = pop_after_sel[i], pop_after_sel[i + 1]\n","            # Crossover and mutation\n","            for c in crossover(p1, p2, crossover_rate):\n","                mutation(c, mutation_rate)\n","                # Store for next generation\n","                children.append(c)\n","\n","        # Replace population\n","        pop_after_mutated = children\n","        population_nextgen = []\n","        for c in pop_after_fit[:2]:\n","            population_nextgen.append(c)\n","        for p in pop_after_mutated:\n","            population_nextgen.append(p)\n","\n","        best_chromo.append(pop_after_fit[0])\n","        best_score.append(score)\n","\n","    return best_chromo, best_score\n","\n","# Running Genetic Algorithm\n","best_chromo, best_score = generations(size=50, n_feat=X.shape[1], crossover_rate=0.8, mutation_rate=0.05, n_gen=500)\n","print(\"Best Chromosome:\", best_chromo)\n","print(\"Best Score:\", best_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0f96EL9Ucin"},"outputs":[],"source":["best_features = best_chromo[0]\n","X_train_best = X.loc[:, best_features]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mxt5XZUhUd8A"},"outputs":[],"source":["final_model = RandomForestClassifier()\n","final_model.fit(X_train_best, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lLqKOoNiUggq"},"outputs":[],"source":["#Save the trained model\n","\n","import joblib\n","\n","joblib.dump(final_model, 'file_model.joblib')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aL6m8t6NUorD"},"outputs":[],"source":["# Assuming you have already loaded the test dataset into `df_test`\n","\n","# Extract test features and target labels\n","df_test = pd.read_csv('file.csv')\n","\n","X_test = df_test.iloc[:, :-1]\n","y_test = df_test[, -1]\n","X_test_best = X_test.loc[:, best_chromo[0]]\n","\n","# Load the final Random Forest model\n","final_model = joblib.load('file_model.joblib')\n","\n","predictions = final_model.predict(X_test_best)\n","\n","# Calculate accuracy or any other evaluation metric\n","accuracy = accuracy_score(y_test, predictions)\n","print(\"Accuracy:\", accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"COb0h9SIVdYZ"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import joblib\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# Visualize feature importances\n","feature_importance = final_model.feature_importances_\n","sorted_importance = np.argsort(feature_importance)[::-1]  # Sort in descending order\n","top_features_indices = sorted_importance[:20]  # Select the top 10 features (you can adjust this number as needed)\n","\n","# Get the names of the top features\n","top_features_names = X.columns[top_features_indices]\n","\n","# Plot feature importances\n","plt.figure(figsize=(10, 6))\n","plt.bar(range(len(top_features_indices)), feature_importance[top_features_indices])\n","plt.xticks(range(len(top_features_indices)), top_features_names, rotation=45)\n","plt.xlabel('Feature')\n","plt.ylabel('Importance')\n","plt.title('Top 20 Features Importance')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-wymLYHVmaW"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import joblib\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import roc_curve, auc\n","\n","# Get the predicted probabilities for class 1 (assuming it's the positive class)\n","y_pred_prob = final_model.predict_proba(X_test_best)[:, 1]\n","\n","# Calculate the ROC curve\n","fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n","\n","# Calculate the AUC (Area Under the Curve)\n","roc_auc = auc(fpr, tpr)\n","\n","# Plot the ROC curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, color='b', label='ROC curve (AUC = %0.2f)' % roc_auc)\n","plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc='lower right')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KxF2tUF4VnJo"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import joblib\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","\n","#Split the data into training and testing sets\n","#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","#Train your classifier (Random Forest in this example)\n","#final_model = RandomForestClassifier()\n","#final_model.fit(X_train, y_train)\n","\n","#Make predictions on the test set\n","y_pred = final_model.predict(X_test_best)\n","\n","# Calculate the confusion matrix\n","cm = confusion_matrix(y_test, y_pred)\n","\n","# Plot the confusion matrix as a heatmap\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.title('Confusion Matrix')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ceeI9SVMV1CD"},"outputs":[],"source":["# Extract the values from the confusion matrix\n","TN, FP, FN, TP = cm.ravel()\n","\n","# Calculate the evaluation metrics\n","accuracy = (TP + TN) / (TP + TN + FP + FN)\n","precision = TP / (TP + FP)\n","recall = TP / (TP + FN)\n","specificity = TN / (TN + FP)\n","f1 = 2 * (precision * recall) / (precision + recall)\n","\n","# Print the results\n","print(\"Accuracy: {:.3f} or {:.1f}%\".format(accuracy, accuracy * 100))\n","print(\"Precision: {:.3f} or {:.1f}%\".format(precision, precision * 100))\n","print(\"Recall (Sensitivity): {:.3f} or {:.1f}%\".format(recall, recall * 100))\n","print(\"Specificity: {:.3f} or {:.1f}%\".format(specificity, specificity * 100))\n","print(\"F1-Score: {:.3f}\".format(f1))\n","\n","# Plot the confusion matrix as a heatmap\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.title('Confusion Matrix')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LtZ8DnDqVwto"},"outputs":[],"source":["#Precision-Recall Curve:\n","\n","from sklearn.metrics import precision_recall_curve\n","import matplotlib.pyplot as plt\n","\n","# Assuming you have already trained your classifier 'final_model' and obtained the predicted probabilities 'y_pred_prob' on the test set\n","precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall, precision, color='blue', label='Precision-Recall Curve')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4EaYME5wV9b2"},"outputs":[],"source":["#Learning Curves:\n","from sklearn.model_selection import learning_curve\n","\n","X = df.iloc[:, :-1]   # Exclude the last column\n","y = df[:, -1]   # Use the last column as the label\n","\n","# Assuming you have already trained your classifier 'final_model'\n","train_sizes, train_scores, test_scores = learning_curve(final_model, X, y, cv=5)\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', color='r', label='Training Accuracy')\n","plt.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', color='g', label='Validation Accuracy')\n","plt.xlabel('Training Size')\n","plt.ylabel('Accuracy')\n","plt.title('Learning Curves')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RR98EZwCWGOV"},"outputs":[],"source":["# Class Distribution:\n","import seaborn as sns\n","\n","# Assuming you have already loaded your dataset as 'X' and 'y' into a DataFrame 'df'\n","plt.figure(figsize=(6, 4))\n","sns.countplot(x='ZTA', data=df)\n","plt.xlabel('Class')\n","plt.ylabel('Count')\n","plt.title('Class Distribution')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6y8ez72WeEZ"},"outputs":[],"source":["#Feature Distribution\n","# Assuming you have already loaded your dataset as 'X' into a DataFrame 'TSS'\n","plt.figure(figsize=(10, 6))\n","for feature in X.columns:\n","    sns.histplot(df[feature], kde=True, label=feature)\n","plt.xlabel('Feature Values')\n","plt.ylabel('Count')\n","plt.title('Feature Distributions')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VtCVDlijWrY-"},"outputs":[],"source":["#Feature Correlation Matrix\n","#Assuming you have already loaded your dataset as 'X' into a DataFrame 'TSS'\n","correlation_matrix = df.corr()\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n","plt.title('Feature Correlation Matrix')\n","plt.show()\n","correlation_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5jQwG65MWzJc"},"outputs":[],"source":["correlation_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mgfucRKpW1fm"},"outputs":[],"source":["#Calibration Curve\n","from sklearn.calibration import calibration_curve\n","\n","# Assuming you have already trained your classifier 'final_model' and obtained the predicted probabilities 'y_pred_prob' on the test set\n","prob_true, prob_pred = calibration_curve(y_test, y_pred_prob, n_bins=10)\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(prob_pred, prob_true, marker='o', linestyle='--', color='blue', label='Calibration Curve')\n","plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n","plt.xlabel('Mean Predicted Probability')\n","plt.ylabel('Fraction of Positives')\n","plt.title('Calibration Curve')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVwiGIZsW6Sa"},"outputs":[],"source":["#Residual Plots (for regression tasks)\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import cross_val_predict\n","\n","# Assuming you have already trained your regression model 'final_model' and obtained predictions 'y_pred' on the test_set\n","# Assuming you have the true regression targets 'y_test' on the test set\n","residuals = y_test - y_pred\n","plt.figure(figsize=(8, 6))\n","plt.scatter(y_pred, residuals, alpha=0.5)\n","plt.axhline(y=0, color='red', linestyle='--')\n","plt.xlabel('Predicted Values')\n","plt.ylabel('Residuals')\n","plt.title('Residual Plot')\n","plt.show()\n","\n","# Calculate the occurrences of each unique value in residuals\n","unique_values, counts = np.unique(residuals, return_counts=True)\n","\n","# Display the results\n","for value, count in zip(unique_values, counts):\n","    print(f\"Occurrences of {value}: {count}\")"]}],"metadata":{"colab":{"provenance":[{"file_id":"1mopAyvJ05gLxma74SQEqrIJarZlSI92j","timestamp":1692566602929},{"file_id":"1xrxNZ4ZBw2_BqSdGiL7Q9S0R54VGxVCv","timestamp":1691517168629},{"file_id":"1pThcYxPTJhQl3lkvzd1cSinC1cXFmIHh","timestamp":1690921561679}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}